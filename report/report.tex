\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{cite}
\usepackage{gensymb}

%opening
\title{Bachelor practical: Report}
\author{Marco UnternÃ¤hrer}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

Training a spiking neural network is a challenging task.
Neuromorphic vision researchers have to create spiking datasets since there is a lack of such datasets publicly available .
One method for converting static image datasets into neuromorphic vision datasets was proposed by moving a sensor in front of an image \cite{Orchard2015}.

Another approach for training a spiking neural network was developed by researchers at the Institute of Neuroinformatics \cite{Diehl2015}.
A non-spiking neural network is trained by using a regular static image dataset like MNIST or Caltech-101.
The resulting weights of the trained neural network can then be used to create a spiking neural network.
One constraint for this to work is that units must have zero bias.\\

The task for this bachelor practical was to train a convolutional neural network on the Caltech-101 dataset.


\section{Image Classification on Caltech-101}
This section will give a short introduction to the Caltech-101 dataset, the used methods and techniques.

\subsection{Caltech-101}
\cite{Fei-Fei2007}


\subsection{Convolutional Neural Network}
A convolutional neural network (CNN) is a type of feed-forward artificial neural network.
CNNs are biologically-inspired variants of multilayer perceptrons \cite{DLCNN}.
From Hubel and Wiesel's early work on the cat's visual cortex \cite{Hubel1968}, it is known that the visual cortex contains a complex arrangement of cells \cite{DLCNN}.
These cells are sensitive to small sub-regions of the visual field, called a receptive field \cite{DLCNN}.
The sub-regions are tiled to cover the entire visual field.
These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images.
Additionally, two basic cell types have been identified: Simple cells respond maximally to specific edge-like patterns within their receptive field.
Complex cells have larger receptive fields and are locally invariant to the exact position of the pattern \cite{DLCNN}.

Like their biological relatives, units in a CNN take advantage of the 2D structure of the input data (images or other 2D input such as a speech signal).
This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features \cite{StanfordTutCNN2015}.

\subsection{Batch Normalization}
bn \cite{Ioffe2015}



\subsection{Cyclical Learning Rates}
In \cite{Bengio2012}, Bengio says that the learning rate is the most important hyper-parameter to optimize and if "there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning".
It is well known that too small a learning rate will make a training algorithm converge slowly while too large a learning rate will make the training algorithm diverge \cite{Zeiler2012}.
Hence one must experiment with a variety of learning rates and schedules (i.e., the timing of learning rate changes) \cite{Smith2015}.

The conventional wisdom is that the learning rate should be a single value that monotonically decreases during the training \cite{Smith2015}.
However, Smith demonstrates the surprising phenomenon that increasing the learning rate is overall beneficial and thus proposes to let the global learning rate vary cyclically within a band of values rather than setting it to a fixed value \cite{Smith2015}.



\section{Experimental Setup}

\subsection{Data Pre-processing \& Data Augmentation}
Since the heights and widths of the Caltech-101 images vary, all images were scaled to a common size. 
Like \cite{Orchard2015}, each image was resized to be as large as possible while maintaining the original aspect ratio and ensuring that width does not exceed 240 pixels and height does not exceed 180 pixels.

After running a few experiments with a fairly deep architecture, it became clear, that the roughly 10'000 images of the Caltech-101 dataset were not enough to accomplish a decent test accuracy.
Starting from the original dataset, data augmentation was pursued by randomly transform each picture ten times, and thus produce approximately 100'000 images.

Using an image data generator \cite{keras}, we ended up with the following augmentation parameters:
\begin{itemize}
	\item rotation: random with angles in the range 0$\degree$ to 20$\degree$
	\item translation: random with shift between 0\% and 20\% of total width/height
	\item flipping: horizontal/vertical, yes or no (Bernoulli)
	\item zooming: random zoom axis by factor +/- 20\%
\end{itemize}

The created images were then resized again, but with the difference that the images were padded with the edge pixels to fill the needed aspect ratio.
The pixel values were normalized to the range of (0, 1) after loading the images.
Data standardization, subtracting the mean and dividing by the standard deviation, has sped up the training and was also applied.\\

The generated images are available online and can be downloaded under \cite{UnternaehrerGenImg}.


\subsection{Architecture}
architectures inspired by Simonyan2015

constraints: 3 convolutional layers, 2 fc


\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline Layer type & Parameters \\ 
			\hline convolution & 128x5x5, stride 2x2 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline convolution & 256x3x3 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline convolution & 512x3x3 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline full &  1024 \\ 
			\hline relu &\\
			\hline full &  102 \\
			\hline softmax & \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Base Network Architecture}
\end{table}


\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline Layer type & Parameters \\ 
			\hline convolution & 128x5x5, stride 2x2 \\ 
			\hline batch normalization & \\
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline convolution & 256x3x3 \\ 
			\hline batch normalization & \\
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline convolution & 512x3x3 \\ 
			\hline batch normalization & \\
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline full &  1024 \\ 
			\hline relu &\\
			\hline full &  102 \\
			\hline softmax & \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Network Architecture with Batch Normalization}
\end{table}

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline Layer type & Parameters \\ 
			\hline convolution & 128x5x5, stride 2x2 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline dropout & 0.35 \\
			\hline convolution & 256x3x3 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline dropout & 0.35 \\
			\hline convolution & 512x3x3 \\ 
			\hline relu &\\
			\hline maxpool &  2x2\\ 
			\hline dropout & 0.35 \\
			\hline full &  1024 \\ 
			\hline relu &\\
			\hline dropout & 0.5 \\
			\hline full &  102 \\
			\hline softmax & \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Network Architecture without Batch Normalization}
\end{table}



\subsection{Training}
train / test split
81378 train samples, 9104 validation samples, 10102 test samples -> train/test split saved to disk
unlike MNIST, caltech-101 doesnt have a predefined split between train/test data

sgd with nesterov

learning rate schedule

\subsection{Implementation Details}
The neural network framework of choice was Keras \cite{keras}.
Keras is a minimalist, highly modular neural networks library, written in Python and running on top of Theano.
Being Python-based and especially developed with a focus on enabling fast experimentation, it was a good candidate for this project.
Most importantly, Keras seemed to provide almost all needed functionality out of the box.
Another relevant aspect, that lead to the decision to use Keras, was that model architectures are created with code.
That made rapid prototyping and experimentation feasible.\\

The networks were trained on a system equipped with a single NVIDIA GTX 970 GPU, and training the best performing net took only ???? to converge.\\

The source code for the entire bachelor practical can be found at \cite{UnternaehrerCode}.

\section{Results}
w/o batch normalization, w/o data normalization
with bn, with data normalization


\section{Conclusion}
enough data is important
bn can speed up training time significantly



\bibliographystyle{abbrv}
\bibliography{C:/Users/Marco/Documents/MendeleyBibtex/Bsc-Practical}
\end{document}
